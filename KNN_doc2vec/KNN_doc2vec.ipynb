{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs,string,re,os\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the dictionaries for title ,abstract and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file=\"H:\\\\MTECH2\\\\IR\\\\IR_Project\\\\Graph\\\\DBLPOnlyCitationOct19.txt\"\n",
    "# fp =codecs.open(file,\"r\",encoding='utf-8',errors='ignore')  \n",
    "# text=fp.read()\n",
    "# text=text.split(\"\\n\\n\")\n",
    "# #print(text)\n",
    "\n",
    "\n",
    "# unique_nodes_list=[]\n",
    "# G_test=pickle.load(open(\"H:\\\\MTECH2\\\\IR\\\\IR_Project\\\\Graph\\\\G_test.p\",\"rb\"))\n",
    "# for single_tuple in G_test:\n",
    "\n",
    "#     unique_nodes_list.append(int(single_tuple[0]))\n",
    "#     unique_nodes_list.append(int(single_tuple[1]))\n",
    "# unique_nodes_list=list(set(unique_nodes_list))\n",
    "\n",
    "# print(len(unique_nodes_list))\n",
    "# title_dict={}\n",
    "# abstract_dict={}\n",
    "# ground_truth_references={}\n",
    "\n",
    "\n",
    "\n",
    "# counter=0\n",
    "# for i in text:\n",
    "#     #print(i)\n",
    "#     counter+=1\n",
    "#     print(counter)\n",
    "#     j = i.split(\"\\n\")\n",
    "#     #print(j)\n",
    "#     #print(1/0)\n",
    "#     abstract=\"\"\n",
    "#     references=[]\n",
    "#     for k in j:\n",
    "#         #print(k)\n",
    "#         if (k.startswith(\"#index\")):\n",
    "#             index = k[6:]\n",
    "#             print(index)\n",
    "#         if (k.startswith(\"#*\")):\n",
    "#             title = k[2:]\n",
    "#         if (k.startswith(\"#!\")):\n",
    "#             abstract = k[2:]\n",
    "#         if (k.startswith(\"#%\")):\n",
    "#             references.append(k[2:])\n",
    "    \n",
    "#     #print(unique_nodes_list)\n",
    "   \n",
    "        \n",
    "#     title_dict[index]=title\n",
    "#     if abstract != \"\":\n",
    "#         abstract_dict[index] = abstract\n",
    "#     ground_truth_references[index] = references\n",
    "    \n",
    "# document_frequency_pickle1 = open(\"title_dict.pickle\",\"wb\")\n",
    "# pickle.dump(title_dict, document_frequency_pickle1)\n",
    "\n",
    "# document_frequency_pickle2 = open(\"abstract_dict.pickle\",\"wb\")\n",
    "# pickle.dump(abstract_dict, document_frequency_pickle2)\n",
    "\n",
    "# document_frequency_pickle3 = open(\"ground_truth_references.pickle\",\"wb\")\n",
    "# pickle.dump(ground_truth_references, document_frequency_pickle3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pickle files for refernces, title and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632442\n",
      "1632442\n",
      "653510\n"
     ]
    }
   ],
   "source": [
    "references=pickle.load(open(\"ground_truth_references.pickle\",\"rb\"))  \n",
    "abstracts=pickle.load(open(\"abstract_dict.pickle\",\"rb\"))  \n",
    "titles=pickle.load(open(\"title_dict.pickle\",\"rb\"))  \n",
    "print(len(references))\n",
    "print(len(titles))\n",
    "print(len(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a lists which contains all the paper ids which refer other paper or referred by any paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475886\n"
     ]
    }
   ],
   "source": [
    "all_titles_list=[]\n",
    "for i in references.keys():\n",
    "    #print(i)\n",
    "    for j in references[i]:\n",
    "        #print(j)\n",
    "        all_titles_list.append(j)\n",
    "        #print(all_titles_list)\n",
    "        \n",
    "#print((set(all_titles_list)))\n",
    "\n",
    "\n",
    "for i in references.keys():\n",
    "    if len(references[i]) !=0:\n",
    "        all_titles_list.append(i)\n",
    "\n",
    "print(len(set(all_titles_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the modified dictionaries of abstracts and titles for paper ids appear in all_tite list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_references={}\n",
    "# new_abstracts={}\n",
    "# new_titles={}\n",
    "\n",
    "# cnt=0\n",
    "# for i in all_titles_list:\n",
    "    \n",
    "#     cnt+=1\n",
    "#     #print(references[i])\n",
    "#     if i in references.keys():\n",
    "#         new_references[i]=references[i]\n",
    "#     if i in abstracts.keys():\n",
    "#         new_abstracts[i]=abstracts[i]\n",
    "#     if i in titles.keys():\n",
    "#         print(cnt)\n",
    "#         new_titles[i]=titles[i]\n",
    "\n",
    "# print(len(new_abstracts))\n",
    "# print(len(new_titles))\n",
    "# print(len(new_references))\n",
    "\n",
    "# document_frequency_pickle1 = open(\"title_modified.pickle\",\"wb\")\n",
    "# pickle.dump(new_titles, document_frequency_pickle1)\n",
    "\n",
    "# document_frequency_pickle2 = open(\"abstract_modified.pickle\",\"wb\")\n",
    "# pickle.dump(new_abstracts, document_frequency_pickle2)\n",
    "\n",
    "# document_frequency_pickle2 = open(\"references_modified.pickle\",\"wb\")\n",
    "# pickle.dump(new_references, document_frequency_pickle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338118\n",
      "475886\n",
      "475886\n"
     ]
    }
   ],
   "source": [
    "abstracts_modified=pickle.load(open(\"abstract_modified.pickle\",\"rb\"))  \n",
    "titles_modified=pickle.load(open(\"title_modified.pickle\",\"rb\"))\n",
    "references_modified=pickle.load(open(\"references_modified.pickle\",\"rb\"))\n",
    "print(len(abstracts_modified))\n",
    "print(len(titles_modified))\n",
    "print(len(references_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an sentences list which is title + abstract for input to tagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sentence_list=[]   \n",
    "# for i in all_titles_list:\n",
    "#     abstract=\"\"\n",
    "#     title=\"\"\n",
    "#     if i in titles_modified.keys():\n",
    "#         title=titles_modified[i]\n",
    "#     if i in abstracts_modified.keys():\n",
    "#         abstract=abstracts_modified[i]\n",
    "#     sentence_list.append(title+\" \"+abstract)\n",
    "\n",
    "# print(\"set len:\",len(sentence_list))\n",
    "\n",
    "# document_frequency_pickle1 = open(\"sentence_list_modified1.pickle\",\"wb\")\n",
    "# pickle.dump(sentence_list, document_frequency_pickle1) \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 475886\n"
     ]
    }
   ],
   "source": [
    "sentence_list=pickle.load(open(\"sentence_list_modified1.pickle\",\"rb\")) \n",
    "print(\"len:\",len(sentence_list)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the tagged data from the sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(sentence_list)]\n",
    "# list_tag=[]\n",
    "# for i in tagged_data:\n",
    "#     list_tag.append(i[1])\n",
    "# #print(list_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the model and building the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "# model = Doc2Vec(size=vec_size,\n",
    "#                 alpha=alpha, \n",
    "#                 min_alpha=0.00025,\n",
    "#                 min_count=1,\n",
    "#                 dm =1)\n",
    "\n",
    "# print(\"model saved\")\n",
    "  \n",
    "# model.build_vocab(tagged_data)\n",
    "# print(\"vocab build\")\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "#     model.train(tagged_data,\n",
    "#                 total_examples=model.corpus_count,\n",
    "#                 epochs=model.iter)\n",
    "#     # decrease the learning rate\n",
    "#     model.alpha -= 0.0002\n",
    "#     # fix the learning rate, no decay\n",
    "#     model.min_alpha = model.alpha\n",
    "\n",
    "# model.save(\"d2v_modified.model\")\n",
    "# print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model and building the samples list for infer vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Doc2VecKeyedVectors'>\n",
      "<gensim.models.keyedvectors.Doc2VecKeyedVectors object at 0x00000131C4156E80>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v_modified.model\")\n",
    "print(type(model.docvecs))\n",
    "print(model.docvecs)\n",
    "samples=[]\n",
    "index_to_sample_map=[]\n",
    "#print(model.docvecs['4990'])\n",
    "# for i in titles_modified.keys():\n",
    "#     #print(i)\n",
    "#     #print(type(i))\n",
    "#     if i in all_titles_list:\n",
    "#         if model.docvecs[i] is not None:\n",
    "#             samples.append(model.docvecs[i])\n",
    "#             index_to_sample_map.append(i)\n",
    "for vector in model.docvecs.vectors_docs:\n",
    "    samples.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the final prediction for the test paper id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper id: 283631\n",
      "original references: 6\n",
      "matched_references: 6\n",
      "0.375\n",
      "1.0\n",
      "0.5454545454545454\n",
      "paper id: 283678\n",
      "original references: 18\n",
      "matched_references: 18\n",
      "0.6428571428571429\n",
      "1.0\n",
      "0.782608695652174\n",
      "paper id: 614290\n",
      "original references: 8\n",
      "matched_references: 8\n",
      "0.4444444444444444\n",
      "1.0\n",
      "0.6153846153846153\n",
      "paper id: 1179195\n",
      "original references: 13\n",
      "matched_references: 13\n",
      "0.5652173913043478\n",
      "1.0\n",
      "0.7222222222222222\n",
      "paper id: 1517610\n",
      "original references: 3\n",
      "matched_references: 3\n",
      "0.23076923076923078\n",
      "1.0\n",
      "0.375\n",
      "paper id: 965353\n",
      "original references: 5\n",
      "matched_references: 5\n",
      "0.3333333333333333\n",
      "1.0\n",
      "0.5\n",
      "paper id: 965505\n",
      "original references: 26\n",
      "matched_references: 26\n",
      "0.7222222222222222\n",
      "1.0\n",
      "0.8387096774193548\n",
      "paper id: 965543\n",
      "original references: 20\n",
      "matched_references: 20\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.8\n",
      "paper id: 588522\n",
      "original references: 5\n",
      "matched_references: 5\n",
      "0.3333333333333333\n",
      "1.0\n",
      "0.5\n",
      "paper id: 588492\n",
      "original references: 10\n",
      "matched_references: 10\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "L1=['283631','283678','614290','1179195','1517610','965353','965505','965543','588522','588492']\n",
    "for i in L1:\n",
    "    #print(i)\n",
    "    #i=str(i)\n",
    "    t=\"\"\n",
    "    a=\"\"\n",
    "    if i in titles_modified.keys():\n",
    "        t=titles_modified[i]\n",
    "    if i in abstracts_modified.keys():\n",
    "        a=abstracts_modified[i]\n",
    "    test_data=t+\"\"+a\n",
    "    test_data = word_tokenize(test_data.lower())\n",
    "    v1 = model.infer_vector(test_data)\n",
    "    neigh = NearestNeighbors(n_neighbors=10)\n",
    "    neigh.fit(samples)\n",
    "    neighbour=(neigh.kneighbors([v1],return_distance=False))\n",
    "\n",
    "    #print(\"neighbour\",neighbour)\n",
    "\n",
    "    final_result=[]\n",
    "    for n in neighbour[0]:\n",
    "        final_result.append(n)\n",
    "        #print(type(i))\n",
    "        n=str(n)\n",
    "        #print(type(i))\n",
    "        if n in references.keys():\n",
    "            #print(i)\n",
    "            #print(references[i])\n",
    "            for j in references[i]:\n",
    "                final_result.append(j)\n",
    "                #a=1\n",
    "    #print(final_result)\n",
    "    final_results=[]\n",
    "    for f in final_result:\n",
    "        final_results.append(str(f)) \n",
    "    #print(final_results)       \n",
    "    cnt=0\n",
    "    gn=references[i]\n",
    "    #print(\"gn\",gn)\n",
    "    final_results=list(set(final_results))\n",
    "    for k in final_results:\n",
    "        #print(type(k))\n",
    "        if k in gn:\n",
    "            cnt+=1\n",
    "    print(\"paper id:\",i)\n",
    "    print(\"original references:\",len(gn))\n",
    "    print(\"matched_references:\",cnt)\n",
    "    \n",
    "    # for calculate the precision and recall\n",
    "    \n",
    "    precision=(len((set(final_results)).intersection(set(gn))))/(len(final_results))\n",
    "    recall=(len((set(final_results)).intersection(set(gn)))/(len(gn)))\n",
    "    f1_score=(2*(precision*recall))/(precision+recall)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
